{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from tqdm.notebook import tqdm\n\nimport numpy as np\nimport pandas as pd\n#pd.set_option('display.max_rows', 500)\n#pd.set_option('display.max_columns', 500)\n#pd.set_option('display.width', 1000)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy\nfrom scipy.stats import probplot\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-28T13:10:50.881155Z","iopub.execute_input":"2021-09-28T13:10:50.881515Z","iopub.status.idle":"2021-09-28T13:10:50.887668Z","shell.execute_reply.started":"2021-09-28T13:10:50.881485Z","shell.execute_reply":"2021-09-28T13:10:50.886710Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## Optiver Realized Volatility Prediction","metadata":{}},{"cell_type":"markdown","source":"## 1. Introduction\n\nThis competition's objective is predicting short-term volatility for 112 stocks across different sectors. Dataset consist book and trade data of the stocks for multiple time buckets. We are supposed to predict a target value (volatility) for every time bucket of the stocks. 107 of the stocks have 3830, 3 of the stocks have 3829, 1 of the stocks has 3820 and 1 of the stocks has 3815 time buckets in training dataset. Thus, there are 428,932 rows to predict in it. Since this is a code competition, public test set is hidden and private test set will be real market data collected in the three-month evaluation period after competition ends. There are 3 columns in training set and 2 columns in placeholder test set.\n\n* `stock_id` - ID of the stock\n* `time_id` - ID of the time bucket\n* `target` - Realized volatility of the next 10 minute window under the same stock_id/time_id (doesn't exist in placeholder test set)","metadata":{}},{"cell_type":"code","source":"train_test_dtypes = {\n    'stock_id': np.uint8,\n    'time_id': np.uint16,\n    'target': np.float64\n}\n\ndf_train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv', dtype=train_test_dtypes)\ndf_test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv', usecols=['stock_id', 'time_id'], dtype=train_test_dtypes)\n\nprint(f'Training Set Shape: {df_train.shape}')\nprint(f'Training Set Memory Usage: {df_train.memory_usage().sum() / 1024 ** 2:.2f} MB')\nprint(f'Test Set Shape: {df_test.shape}')\nprint(f'Test Set Memory Usage: {df_test.memory_usage().sum() / 1024 ** 2:.2f} MB')","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:10:50.889312Z","iopub.execute_input":"2021-09-28T13:10:50.889686Z","iopub.status.idle":"2021-09-28T13:10:51.170558Z","shell.execute_reply.started":"2021-09-28T13:10:50.889656Z","shell.execute_reply":"2021-09-28T13:10:51.169326Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:10:51.172534Z","iopub.execute_input":"2021-09-28T13:10:51.172897Z","iopub.status.idle":"2021-09-28T13:10:51.194622Z","shell.execute_reply.started":"2021-09-28T13:10:51.172864Z","shell.execute_reply":"2021-09-28T13:10:51.193707Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## 2. Evaluation\n\nSubmissions are scored using RMSPE (root mean squared percentage error) which can be denoted as\n\n$\\huge \\text{RMSPE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} ((y_i - \\hat{y}_i)/y_i)^2}$\n\nRMSPE is very similar to RMSE. The only difference between them is, the error is divided by the actual value. Predictions closer to actual values yield errors closer to 0, so division by actual values is sensitive to larger errors. In addition to that, errors are squared before they are averaged, which makes this metric even more sensitive to larger errors. This means larger errors are not tolerable in this domain.\n\nOne pitfall of RMSPE is it can raise ZeroDivisionError if a single data point in actual values is equal to 0. Even though, there isn't any 0 target values in training set, this can be easily solved by adding a small constant to actual values. A small constant epsilon wouldn't contribute to overall rmspe and it prevents ZeroDivisionError.\n\nFor baselines, global target mean scores **1.11033**, target mean of stock_ids scores **0.789618**, and target median of stock_ids scores **0.589135**  on training set.","metadata":{}},{"cell_type":"code","source":"def root_mean_squared_percentage_error(y_true, y_pred, epsilon=1e-10):\n    \n    rmspe = np.sqrt(np.mean(np.square((y_true - y_pred) / (y_true + epsilon))))\n    return rmspe\n\n\ntarget_mean_rmspe = root_mean_squared_percentage_error(df_train['target'], np.repeat(df_train['target'].mean(), len(df_train)))\nprint(f'target Mean RMPSE: {target_mean_rmspe:.6}')\n\nstock_id_target_mean_rmspe = root_mean_squared_percentage_error(df_train['target'], df_train.groupby('stock_id')['target'].transform('mean'))\nprint(f'stock_id target Mean RMPSE: {stock_id_target_mean_rmspe:.6}')\n\nstock_id_target_median_rmspe = root_mean_squared_percentage_error(df_train['target'], df_train.groupby('stock_id')['target'].transform('median'))\nprint(f'stock_id target Median RMPSE: {stock_id_target_median_rmspe:.6}')","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:10:51.196139Z","iopub.execute_input":"2021-09-28T13:10:51.196450Z","iopub.status.idle":"2021-09-28T13:10:51.293048Z","shell.execute_reply.started":"2021-09-28T13:10:51.196420Z","shell.execute_reply":"2021-09-28T13:10:51.291943Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## 3. Realized Volatility (Target)\n\nTarget is defined as realized volatility computed over the 10 minute window following the feature data under the same stock/time_id. For every stock_id and its time_id, target is the next 10 minute realized volatility of the same stock_id and time_id. Thus, there are 10 minute gaps between provided dataset and target values, and there is no overlap between them.\n\nRealized volatility, $\\sigma$, is the squared root of the sum of squared log returns which is denoted as\n\n$\\huge \\sigma = \\sqrt{\\sum_{t}r_{t-1, t}^2}$\n\nand log return of a stock $S$ at time $t$ is\n\n$\\huge r_{t-1, t} = \\log \\left( \\frac{S_{t-1}}{S_{t1}} \\right)$\n\nwhere $S_t$ is the price of the stock $S$ at time $t$. The price used for targets is the weighted averaged price (WAP) and it can be derived from book data. Log returns are used for realized volatility calculation because price differences are not always comparable across stocks.","metadata":{}},{"cell_type":"code","source":"def visualize_target(target, df_train):\n    \n    print(f'{target}\\n{\"-\" * len(target)}')\n        \n    print(f'Mean: {df_train[target].mean():.4f}  -  Median: {df_train[target].median():.4f}  -  Std: {df_train[target].std():.4f}')\n    print(f'Min: {df_train[target].min():.4f}  -  25%: {df_train[target].quantile(0.25):.4f}  -  50%: {df_train[target].quantile(0.5):.4f}  -  75%: {df_train[target].quantile(0.75):.4f}  -  Max: {df_train[target].max():.4f}')\n    print(f'Skew: {df_train[target].skew():.4f}  -  Kurtosis: {df_train[target].kurtosis():.4f}')\n    missing_values_count = df_train[df_train[target].isnull()].shape[0]\n    training_samples_count = df_train.shape[0]\n    print(f'Missing Values: {missing_values_count}/{training_samples_count} ({missing_values_count * 100 / training_samples_count:.4f}%)')\n\n    fig, axes = plt.subplots(ncols=1, figsize=(12, 8), dpi=100)\n    sns.distplot(df_train[target], label=target, kde=True, ax=axes)\n    axes.axvline(df_train[target].mean(), label=f'{target} Mean', color='r', linewidth=2, linestyle='--')\n    axes.axvline(df_train[target].median(), label=f'{target} Median', color='b', linewidth=2, linestyle='--')\n    axes.legend(prop={'size': 16})\n    \n\n    axes.set_title(f'{target} Distribution in Training Set', fontsize=20, pad=15)\n    \n\n    plt.show()\n","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-09-28T13:10:51.294226Z","iopub.execute_input":"2021-09-28T13:10:51.294512Z","iopub.status.idle":"2021-09-28T13:10:51.304664Z","shell.execute_reply.started":"2021-09-28T13:10:51.294484Z","shell.execute_reply":"2021-09-28T13:10:51.303505Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"visualize_target('target',df_train)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:10:51.306103Z","iopub.execute_input":"2021-09-28T13:10:51.306435Z","iopub.status.idle":"2021-09-28T13:10:53.756712Z","shell.execute_reply.started":"2021-09-28T13:10:51.306403Z","shell.execute_reply":"2021-09-28T13:10:53.755696Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"* Distribution of target for specific stocks.","metadata":{}},{"cell_type":"code","source":"visualize_target('target',df_train[df_train['stock_id']==0])","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:10:53.758058Z","iopub.execute_input":"2021-09-28T13:10:53.758367Z","iopub.status.idle":"2021-09-28T13:10:54.219334Z","shell.execute_reply.started":"2021-09-28T13:10:53.758337Z","shell.execute_reply":"2021-09-28T13:10:54.218399Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"visualize_target('target',df_train[df_train['stock_id']==37])","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:10:54.222380Z","iopub.execute_input":"2021-09-28T13:10:54.223078Z","iopub.status.idle":"2021-09-28T13:10:54.804972Z","shell.execute_reply.started":"2021-09-28T13:10:54.223026Z","shell.execute_reply":"2021-09-28T13:10:54.803984Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"* Very similar to a Log normal: $\\frac{1}{\\sqrt{2\\pi}x\\sigma} e^{-\\frac{(\\ln x -\\mu)^2}{2\\sigma^2}}$","metadata":{}},{"cell_type":"code","source":"# log normal\nx = np.arange(0.01,10,0.01)\ndef log_normal(x, mu, std):\n    return np.exp(-(np.log(x)-mu)**2 /(2*std**2) ) / (x*std*np.sqrt(2*np.pi))\nmu=0; std=1\nsample = log_normal(x,mu,std)\n\naxes = plt.subplots(ncols=1, figsize=(12, 8), dpi=100)\naxes = plt.plot(x,sample)\nplt.axvline(np.exp(mu+std**2/2), label=f'Mean', color='r', linewidth=2, linestyle='--')\nplt.axvline(np.exp(mu), label=f'Median', color='b', linewidth=2, linestyle='--')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:10:54.807406Z","iopub.execute_input":"2021-09-28T13:10:54.807818Z","iopub.status.idle":"2021-09-28T13:10:55.059302Z","shell.execute_reply.started":"2021-09-28T13:10:54.807783Z","shell.execute_reply":"2021-09-28T13:10:55.058263Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"* Fréchet distribution, since at each day, the WAP is derived from book price that are maximum bet and minimum ask price. It might have a extreme value distribution. Fréchet distribution density: $\\frac{\\alpha}{s} \\; \\left(\\frac{x-m}{s}\\right)^{-1-\\alpha} \\; e^{-(\\frac{x-m}{s})^{-\\alpha}}$\n\n* It is no where close to a chi distribution","metadata":{}},{"cell_type":"code","source":"# sample = 0.001*np.sqrt(np.random.noncentral_chisquare(1000,4,600))\n\n# axes = plt.subplots(ncols=1, figsize=(12, 8), dpi=100)\n# axes= sns.histplot(sample, kde=True)\n# #\n# axes.axvline(sample.mean(), label=f'Mean', color='r', linewidth=2, linestyle='--')\n# axes.axvline(np.median(sample), label=f'Median', color='b', linewidth=2, linestyle='--')\n# axes.legend(prop={'size': 16})\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:10:55.060661Z","iopub.execute_input":"2021-09-28T13:10:55.060978Z","iopub.status.idle":"2021-09-28T13:10:55.064739Z","shell.execute_reply.started":"2021-09-28T13:10:55.060950Z","shell.execute_reply":"2021-09-28T13:10:55.063845Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"Each stock is very different in terms of volatility. Stocks are displayed below from most volatile to least volatile. They are ranked by their mean target value across multiple time_ids. Wide error bars show that even the realized volatility is volatile between multiple time buckets. One stock can be extremely volatile in one time bucket and less volatile in another time bucket. This phenomenon can be explained by time_ids not being sequential, and there aren't any temporal dependencies between them.","metadata":{}},{"cell_type":"code","source":"df_train.groupby('stock_id')['target']","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:10:55.066223Z","iopub.execute_input":"2021-09-28T13:10:55.066565Z","iopub.status.idle":"2021-09-28T13:10:55.081233Z","shell.execute_reply.started":"2021-09-28T13:10:55.066479Z","shell.execute_reply":"2021-09-28T13:10:55.080126Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"target_means = df_train.groupby('stock_id')['target'].mean()\ntarget_stds = df_train.groupby('stock_id')['target'].std()\n\ntarget_means_and_stds = pd.concat([target_means, target_stds], axis=1)\ntarget_means_and_stds.columns = ['mean', 'std']\ntarget_means_and_stds.sort_values(by='mean', ascending=True, inplace=True)\n\nfig, ax = plt.subplots(figsize=(32, 48))\nax.barh(\n    y=np.arange(len(target_means_and_stds)),\n    width=target_means_and_stds['mean'],\n    xerr=target_means_and_stds['std'],\n    align='center',\n    ecolor='black',\n    capsize=3\n)\n\nax.set_yticks(np.arange(len(target_means_and_stds)))\nax.set_yticklabels(target_means_and_stds.index)\nax.set_xlabel('target', size=20, labelpad=15)\nax.set_ylabel('stock_id', size=20, labelpad=15)\nax.tick_params(axis='x', labelsize=20, pad=10)\nax.tick_params(axis='y', labelsize=20, pad=10)\nax.set_title('Mean Realized Volatility of Stocks', size=25, pad=20)\n\nplt.show()\n\ndel target_means, target_stds, target_means_and_stds","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-28T13:10:55.082822Z","iopub.execute_input":"2021-09-28T13:10:55.083211Z","iopub.status.idle":"2021-09-28T13:10:56.827901Z","shell.execute_reply.started":"2021-09-28T13:10:55.083114Z","shell.execute_reply":"2021-09-28T13:10:56.826742Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"Instead of entire stocks, individual time buckets from different stocks are ranked based on their realized volatility. \n* The most volatile 10 time buckets: The most volatile time bucket belongs to stock 77 and its time_id is 24600. The most volatile stock was stock 18 and it has 3 time buckets in this list.\n\n* The least volatile 10 time buckets: All of the least volatile 10 time buckets belong to stock 31, even though it has an average volatility overall. This could be an anomaly and it must be explored further.","metadata":{}},{"cell_type":"code","source":"df_train['stock_time_id'] = df_train['stock_id'].astype(str) + '_' + df_train['time_id'].astype(str)\n\n#Top 10 Most Volatile Time Buckets\nfig, ax = plt.subplots(figsize=(32, 10))\nax.barh(\n    y=np.arange(10),\n    width=df_train.sort_values(by='target', ascending=True).tail(10)['target'],\n    align='center',\n    ecolor='black',\n)\n\nax.set_yticks(np.arange(10))\nax.set_yticklabels(df_train.sort_values(by='target', ascending=True).tail(10)['stock_time_id'])\nax.set_xlabel('target', size=20, labelpad=15)\nax.set_ylabel('stock_time_id', size=20, labelpad=15)\nax.tick_params(axis='x', labelsize=20, pad=10)\nax.tick_params(axis='y', labelsize=20, pad=10)\nax.set_title('Top 10 Most Volatile Time Buckets', size=25, pad=20)\n\nplt.show()\n\n# Top 10 Least Volatile Time Buckets\nfig, ax = plt.subplots(figsize=(32, 10))\nax.barh(\n    y=np.arange(10),\n    width=df_train.sort_values(by='target', ascending=True).head(10)['target'],\n    align='center',\n    ecolor='black',\n)\n\nax.set_yticks(np.arange(10))\nax.set_yticklabels(df_train.sort_values(by='target', ascending=True).head(10)['stock_time_id'])\nax.set_xlabel('target', size=20, labelpad=15)\nax.set_ylabel('stock_time_id', size=20, labelpad=15)\nax.tick_params(axis='x', labelsize=20, pad=10)\nax.tick_params(axis='y', labelsize=20, pad=10)\nax.set_title('Top 10 Least Volatile Time Buckets', size=25, pad=20)\n\nplt.show()\n\ndf_train.drop(columns=['stock_time_id'], inplace=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-28T13:10:56.829348Z","iopub.execute_input":"2021-09-28T13:10:56.829692Z","iopub.status.idle":"2021-09-28T13:10:59.202743Z","shell.execute_reply.started":"2021-09-28T13:10:56.829658Z","shell.execute_reply":"2021-09-28T13:10:59.201640Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"### Correlations of target is very high\nstock_id 12 is not present;\nstock_id 13,38,75,80,100 have different length.","metadata":{}},{"cell_type":"code","source":"df_train_tar =pd.DataFrame(index=df_train.time_id.unique())\nfor i in df_train.stock_id.unique():\n    try:\n        df_train_tar[i] = df_train[df_train['stock_id']==i]['target'].values\n    except ValueError as e:\n        print(i, e)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:10:59.204569Z","iopub.execute_input":"2021-09-28T13:10:59.205013Z","iopub.status.idle":"2021-09-28T13:10:59.390510Z","shell.execute_reply.started":"2021-09-28T13:10:59.204958Z","shell.execute_reply":"2021-09-28T13:10:59.389399Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"def corr_matrix(df,threshold=0.4):\n    #we color correlation > threshold\n    cor = df.corr().round(2)\n    np.fill_diagonal(cor.values, 0)\n    Bigcor = (cor.abs()> threshold ).any(1)[(cor.abs()> threshold ).any(1)==True].index\n    cor =(df[Bigcor]).corr().round(2).abs()\n    # generate mask to filter out any number < threshold\n    mask = np.triu(np.ones_like(cor, dtype=np.bool))+ np.array(cor.abs()<threshold)\n    sns.set(font_scale=0.8)\n    plt.figure(figsize=(19, 15))\n    sns.heatmap(cor, annot=True, fmt='.1f', cmap='coolwarm', mask=mask, linewidths=1, cbar=True)\n    plt.xticks(rotation=30, ha=\"right\") \n    plt.title('Linear correlaton', fontsize=16)\n    plt.show()\n    \ncorr_matrix(df_train_tar.iloc[:,10:30],threshold=0.4)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:10:59.392015Z","iopub.execute_input":"2021-09-28T13:10:59.392432Z","iopub.status.idle":"2021-09-28T13:11:00.678938Z","shell.execute_reply.started":"2021-09-28T13:10:59.392387Z","shell.execute_reply":"2021-09-28T13:11:00.677932Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"* use index to combine those columns with shorter time_id's.","metadata":{}},{"cell_type":"code","source":"df_train_tar = pd.DataFrame(index=df_train.time_id.unique())\nfor i in df_train.stock_id.unique():\n    try:\n        df_train_tar[i] = df_train[df_train['stock_id']==i][['time_id','target']].set_index('time_id')\n    except ValueError as e:\n        print(i, e)             \n","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:11:00.680445Z","iopub.execute_input":"2021-09-28T13:11:00.680819Z","iopub.status.idle":"2021-09-28T13:11:01.159109Z","shell.execute_reply.started":"2021-09-28T13:11:00.680788Z","shell.execute_reply":"2021-09-28T13:11:01.158005Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"df_train_tar[df_train_tar[13].isnull()][13]","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:11:01.160394Z","iopub.execute_input":"2021-09-28T13:11:01.160731Z","iopub.status.idle":"2021-09-28T13:11:01.170908Z","shell.execute_reply.started":"2021-09-28T13:11:01.160700Z","shell.execute_reply":"2021-09-28T13:11:01.169814Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"corr_matrix(df_train_tar.iloc[:,65:90],threshold=0.4)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:11:01.172522Z","iopub.execute_input":"2021-09-28T13:11:01.172975Z","iopub.status.idle":"2021-09-28T13:11:02.946210Z","shell.execute_reply.started":"2021-09-28T13:11:01.172940Z","shell.execute_reply":"2021-09-28T13:11:02.945226Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"plt.scatter(df_train_tar[0], df_train_tar[1])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:11:02.948422Z","iopub.execute_input":"2021-09-28T13:11:02.948746Z","iopub.status.idle":"2021-09-28T13:11:03.199917Z","shell.execute_reply.started":"2021-09-28T13:11:02.948717Z","shell.execute_reply":"2021-09-28T13:11:03.198658Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"def column_to_time_stock_matrix(df, col_name):\n    # turn a column df_train[col_name] to a time * stocks matrix\n    return df.pivot(index='time_id', columns='stock_id', values= col_name)\n\ndef corr_matrix(df, threshold=0.4, drop_low = False):\n    #we only keep correlation > threshold\n    cor =df.corr().round(2)\n    if drop_low: # drop columns that have all correlations < threshold\n        large_col = (cor.abs()> threshold ).any(1)[(cor.abs()> threshold ).any(1)==True].index\n        cor =(df[large_col]).corr().round(2).abs()\n    return cor\ndef see_matrix(cor,threshold=0.4):\n    # generate mask to filter out any number < threshold\n    np.fill_diagonal(cor.values, 0)\n    mask = np.triu(np.ones_like(cor, dtype=np.bool))+ np.array(cor.abs()<threshold)\n    sns.set(font_scale=0.8)\n    fig = plt.figure(figsize=(19, 15))\n    sns.heatmap(cor, annot=True, fmt='.2f', cmap='coolwarm', mask=mask, linewidths=1, cbar=True)\n    plt.xticks(rotation=30, ha=\"right\") \n    #plt.show()\n    np.fill_diagonal(cor.values, 1)\n\n\n# turn target and prediction columns to matrix\n# df_reVol =  column_to_time_stock_matrix(test, 'log_return1_sqrt_sum_square')\ndf_tar = column_to_time_stock_matrix(df_train, 'target')\n\n\n# Stock correlation\ncorr =corr_matrix(df_tar)   \nsee_matrix(corr,threshold=0.8)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:11:03.201273Z","iopub.execute_input":"2021-09-28T13:11:03.201611Z","iopub.status.idle":"2021-09-28T13:11:20.146254Z","shell.execute_reply.started":"2021-09-28T13:11:03.201537Z","shell.execute_reply":"2021-09-28T13:11:20.145455Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"* correlation - Clustering","metadata":{}},{"cell_type":"code","source":"from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\nfrom scipy.spatial.distance import squareform\n\n\ndissimilarity = 1 - abs(df_train_tar.corr())\nZ = linkage(squareform(dissimilarity), 'complete')\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:11:20.147423Z","iopub.execute_input":"2021-09-28T13:11:20.147901Z","iopub.status.idle":"2021-09-28T13:11:20.285070Z","shell.execute_reply.started":"2021-09-28T13:11:20.147869Z","shell.execute_reply":"2021-09-28T13:11:20.284240Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\ndendrogram(Z, labels=df_train_tar.columns, orientation='top', \n           leaf_rotation=90);\nax = plt.gca()\nax.tick_params(axis='x', which='major', labelsize=12)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:11:20.286321Z","iopub.execute_input":"2021-09-28T13:11:20.286819Z","iopub.status.idle":"2021-09-28T13:11:23.779203Z","shell.execute_reply.started":"2021-09-28T13:11:20.286787Z","shell.execute_reply":"2021-09-28T13:11:23.778335Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"## 4. Order Book\n\nOrder book is a list of buy and sell orders organized by price level for given stocks. An order book lists the number of shares being bid on or asked at each price point. Order books help to improve market transparency as they provide information on price, availability and depth of trade.\n\nOrder book data files are named as `book_train.parquet` and `book_test.parquet`, and they are parquet files partitioned by stock_id column. Partitioned by stock_id means those files can both read as whole or individual stocks. However, reading them as a single file is not easy as they will consume too much memory.\n\nThere are 10 columns in every book data partition. The columns are:\n\n* `time_id` - ID of the time bucket\n* `seconds_in_bucket` - Number of seconds passed since the start of the bucket\n* `bid_price1` - Highest buy price after normalization\n* `ask_price1` - Lowest sell price after normalization\n* `bid_price2` - Second highest buy price after normalization\n* `ask_price2` - Second lowest sell price after normalization\n* `bid_size1` - Number of shares on the highest buy price\n* `ask_size1` - Number of shares on the lowest sell price\n* `bid_size2` - Number of shares on the second highest buy price\n* `ask_size2` - Number of shares on the second lowest sell price\n\nValues in order book are the last snapshots of each second. Some of the seconds are not available because there weren't any related market activities during those seconds, thus order books are not updated. Normally, given order books last 600 seconds for every time bucket. Values of the missing seconds are the values at the last updated second, so the book data can be reindexed to 600 seconds for every time bucket and missing values can be forward filled for every field. Forward filling and sorting order books by time_id and seconds_in_bucket doesn't change the extracted features but these functionalities can be added for private test set as a sanity check.","metadata":{}},{"cell_type":"markdown","source":"As mentioned, realized volatilities are calculated from the weighted averaged price in order books of every stock. The formula for weighted averaged price is\n\n$\\large WAP = \\frac{BidPrice_{1}*AskSize_{1} + AskPrice_{1}*BidSize_{1}}{BidSize_{1} + AskSize_{1}}$\n\nRealized volatilities are calculated using the most competitive buy and sell levels, but same formula can applied to second most competitive buy and sell levels or other prices/sizes as well. After that, log returns of the weighted averaged price are summed and square rooted for realized volatility calculation.\n\nRealized volatility, $\\sigma$, is the squared root of the sum of squared log returns which is denoted as $ \\sigma = \\sqrt{\\sum_{t}r_{t-1, t}^2}$and log return of a stock $S$ at time $t$ is $ r_{t-1, t} = \\log \\left( \\frac{S_{t-1}}{S_{t1}} \\right)$  where $S_t$ is the price of the stock $S$ at time $t$.","metadata":{}},{"cell_type":"code","source":"def read_book_data(dataset, stock_id, short=False, sort=True, forward_fill=False):\n    book_dtypes = {\n        'time_id': np.uint16,\n        'seconds_in_bucket': np.uint16,\n        'bid_price1': np.float32,\n        'ask_price1': np.float32,\n        'bid_size1': np.uint32,\n        'ask_size1': np.uint32,\n    }\n    if not short: # want more information\n        book_dtypes= {**book_dtypes,\n                        'bid_price2': np.float32,\n                        'ask_price2': np.float32,\n                        'bid_size2': np.uint32,\n                        'ask_size2': np.uint32,}\n\n    df_book = pd.read_parquet(f'../input/optiver-realized-volatility-prediction/book_{dataset}.parquet/stock_id={stock_id}',\n                              columns = book_dtypes.keys())\n\n    for column, dtype in book_dtypes.items():\n        df_book[column] = df_book[column].astype(dtype)\n    \n    if sort:\n        df_book.sort_values(by=['time_id', 'seconds_in_bucket'], inplace=True)\n        \n    if forward_fill:\n        df_book = df_book.set_index(['time_id', 'seconds_in_bucket'])\n        df_book = df_book.reindex(pd.MultiIndex.from_product([df_book.index.levels[0], np.arange(0, 600)], names=['time_id', 'seconds_in_bucket']), method='ffill')\n        df_book.reset_index(inplace=True)\n\n    return df_book\n\n# read_book_data('train', 1,short=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:11:23.783209Z","iopub.execute_input":"2021-09-28T13:11:23.783664Z","iopub.status.idle":"2021-09-28T13:11:23.793729Z","shell.execute_reply.started":"2021-09-28T13:11:23.783632Z","shell.execute_reply":"2021-09-28T13:11:23.792533Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"read_book_data('train', 1,short=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:11:23.796374Z","iopub.execute_input":"2021-09-28T13:11:23.796756Z","iopub.status.idle":"2021-09-28T13:11:24.678895Z","shell.execute_reply.started":"2021-09-28T13:11:23.796683Z","shell.execute_reply":"2021-09-28T13:11:24.678217Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"def save_log_return(df_train):\n    for stock_id in tqdm(sorted(df_train['stock_id'].unique())):\n\n        df_book = read_book_data('train', stock_id)\n\n        # Weighted averaged prices\n        df_book['wap1'] = (df_book['bid_price1'] * df_book['ask_size1'] + df_book['ask_price1'] * df_book['bid_size1']) /\\\n                          (df_book['bid_size1'] + df_book['ask_size1']).astype('float32')\n        df_book['wap2'] = (df_book['bid_price2'] * df_book['ask_size2'] + df_book['ask_price2'] * df_book['bid_size2']) /\\\n                          (df_book['bid_size2'] + df_book['ask_size2']).astype('float32')\n\n        # Realized volatilities rv1, rv2\n        for wap in [1, 2]:\n            df_book[f'squared_log_return_from_wap{wap}'] = (np.log(df_book[f'wap{wap}'] / df_book.groupby('time_id')[f'wap{wap}'].shift(1)) ** 2).astype('float32')\n             # save data for deeper exploration\n        df_book[['time_id','seconds_in_bucket',f'squared_log_return_from_wap1',f'squared_log_return_from_wap2']].to_parquet(f'log_return_{stock_id}.parquet')\n    return 0\nSAVED = True\nif not SAVED:\n    save_log_return(df_train)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:11:24.679853Z","iopub.execute_input":"2021-09-28T13:11:24.680216Z","iopub.status.idle":"2021-09-28T13:11:24.688007Z","shell.execute_reply.started":"2021-09-28T13:11:24.680189Z","shell.execute_reply":"2021-09-28T13:11:24.687138Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# this will use the log_returns saved in above function to generate features as columns in df_train\ndef feature_from_log_return(df_train, old_fea =True, split=5):\n    \n    for stock_id in tqdm(sorted(df_train['stock_id'].unique())):\n        df_book = pd.read_parquet(f'../input/optiver-log-return/log_return_{stock_id}.parquet') # (f'log_return_{stock_id}.parquet')\n        \n        if old_fea: # put old feature under if, so it will not be generated repeatedly in next run\n            # Realized volatilities features rv1 from wap1, rv2 from wap2\n            for wap in [1, 2]:\n                realized_volatilities = np.sqrt(df_book.groupby('time_id')[f'squared_log_return_from_wap{wap}'].sum()).to_dict()  \n                df_train.loc[df_train['stock_id'] == stock_id, f'rv{wap}']= df_train[df_train['stock_id'] == stock_id]['time_id'].map(realized_volatilities).astype('float32')\n        \n        # realized volatilities from only wap1, but split \n        # at 0_split (time < 60* split)\n        realized_volatilities = np.sqrt(df_book[df_book['seconds_in_bucket']<60*split].groupby('time_id')['squared_log_return_from_wap1'].sum()).to_dict()\n        df_train.loc[df_train['stock_id'] == stock_id, f'rv_0_{split}']= df_train[df_train['stock_id'] == stock_id]['time_id'].map(realized_volatilities).astype('float32')\n        # at 0_split (time >= 60* split)\n        realized_volatilities = np.sqrt(df_book[df_book['seconds_in_bucket']>=60*split].groupby('time_id')['squared_log_return_from_wap1'].sum()).to_dict()\n        df_train.loc[df_train['stock_id'] == stock_id, f'rv_{split}_10']= df_train[df_train['stock_id'] == stock_id]['time_id'].map(realized_volatilities).astype('float32')\n    \n    # save df_train\n    df_train.to_parquet('train.parquet')\n    return df_train\nSAVED = True\nif SAVED: \n    df_train = pd.read_parquet('../input/optiver-log-return/train.parquet')\nelse:\n    df_train = feature_from_log_return(df_train, old_fea = True, split=5)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:11:24.689243Z","iopub.execute_input":"2021-09-28T13:11:24.689522Z","iopub.status.idle":"2021-09-28T13:11:25.143325Z","shell.execute_reply.started":"2021-09-28T13:11:24.689497Z","shell.execute_reply":"2021-09-28T13:11:25.142412Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:11:25.144519Z","iopub.execute_input":"2021-09-28T13:11:25.144820Z","iopub.status.idle":"2021-09-28T13:11:25.158113Z","shell.execute_reply.started":"2021-09-28T13:11:25.144793Z","shell.execute_reply":"2021-09-28T13:11:25.157087Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"def feature_error(feature):\n    RMSPE = root_mean_squared_percentage_error(df_train['target'], df_train[feature])\n    print(f'{feature} RMPSE: {RMSPE:.6}')\n\n# Realized volatilities calculated from WAP1 scores **0.341354** \nfeature_error('rv1')\n# Realized volatilities calculated from WAP2 scores **0.705453** \nfeature_error('rv2')\n# Realized volatilities calculated from WAP1 0-5mins scores **0.347109** \nfeature_error('rv_0_5')\n# Realized volatilities calculated from WAP1 5-10mins scores **0.340067** \nfeature_error('rv_5_10')","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:11:25.159474Z","iopub.execute_input":"2021-09-28T13:11:25.159792Z","iopub.status.idle":"2021-09-28T13:11:25.188486Z","shell.execute_reply.started":"2021-09-28T13:11:25.159765Z","shell.execute_reply":"2021-09-28T13:11:25.187479Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"## correlations between features and target","metadata":{}},{"cell_type":"code","source":"def column_to_time_stock_matrix(df_train, col_name):\n    # turn a column df_train[col_name] to a time * stocks matrix\n    df = pd.DataFrame(index=df_train.time_id.unique())\n    for i in df_train.stock_id.unique():\n        try:\n            df[i] = df_train[df_train['stock_id']==i][['time_id',col_name]].set_index('time_id')\n        except ValueError as e:\n            print(i, e)  \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:11:25.189807Z","iopub.execute_input":"2021-09-28T13:11:25.190123Z","iopub.status.idle":"2021-09-28T13:11:25.196396Z","shell.execute_reply.started":"2021-09-28T13:11:25.190092Z","shell.execute_reply":"2021-09-28T13:11:25.195307Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"# turn target column to matrix\ndf_tar = column_to_time_stock_matrix(df_train, 'target')\n\n# turn feature columns to matrix \nfea_col = df_train.columns[3:]\nprint('features: ', fea_col)\nfeatures = {}\ndf_corr= pd.DataFrame()\nfor feat in fea_col:\n    features[feat] = column_to_time_stock_matrix(df_train, feat)\n    df_corr['targ_'+feat] = df_tar.corrwith(features[feat])\nsns.displot(df_corr, kde=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:11:25.198628Z","iopub.execute_input":"2021-09-28T13:11:25.199069Z","iopub.status.idle":"2021-09-28T13:11:27.717914Z","shell.execute_reply.started":"2021-09-28T13:11:25.199026Z","shell.execute_reply":"2021-09-28T13:11:27.716848Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"## correlations between different stocks for each feature","metadata":{}},{"cell_type":"code","source":"def corr_matrix(df, threshold=0.4, drop_low = False):\n    #we only keep correlation > threshold\n    cor =df.corr().round(2)\n    if drop_low: # drop columns that have all correlations < threshold\n        large_col = (cor.abs()> threshold ).any(1)[(cor.abs()> threshold ).any(1)==True].index\n        cor =(df[large_col]).corr().round(2).abs()\n    return cor\ndef see_matrix(cor,threshold=0.4):\n    # generate mask to filter out any number < threshold\n    np.fill_diagonal(cor.values, 0)\n    mask = np.triu(np.ones_like(cor, dtype=np.bool))+ np.array(cor.abs()<threshold)\n    sns.set(font_scale=0.8)\n    plt.figure(figsize=(19, 15))\n    sns.heatmap(cor, annot=True, fmt='.2f', cmap='coolwarm', mask=mask, linewidths=1, cbar=True)\n    plt.xticks(rotation=30, ha=\"right\") \n    plt.title('Linear correlaton', fontsize=16)\n    plt.show()\n    np.fill_diagonal(cor.values, 1)\n\ncorr =corr_matrix(df_tar.loc[:,:])   \nsee_matrix(corr,threshold=0.8)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:11:27.719092Z","iopub.execute_input":"2021-09-28T13:11:27.719393Z","iopub.status.idle":"2021-09-28T13:11:41.576700Z","shell.execute_reply.started":"2021-09-28T13:11:27.719366Z","shell.execute_reply":"2021-09-28T13:11:41.575717Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\ndendrogram(Z, labels=df_train_tar.columns, orientation='top', \n           leaf_rotation=90);\nax = plt.gca()\nax.tick_params(axis='x', which='major', labelsize=12)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:11:41.578053Z","iopub.execute_input":"2021-09-28T13:11:41.578362Z","iopub.status.idle":"2021-09-28T13:11:44.830423Z","shell.execute_reply.started":"2021-09-28T13:11:41.578332Z","shell.execute_reply":"2021-09-28T13:11:44.829654Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"corr[[3,8,17,20,40,80,81,95,102,110,123]]\n#corr[123].sort_values(ascending=True)\n#corr[8].sort_values(ascending=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:11:44.831515Z","iopub.execute_input":"2021-09-28T13:11:44.831963Z","iopub.status.idle":"2021-09-28T13:11:44.860501Z","shell.execute_reply.started":"2021-09-28T13:11:44.831917Z","shell.execute_reply":"2021-09-28T13:11:44.859618Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"- Difference of correlation matrix is small","metadata":{}},{"cell_type":"code","source":"see_matrix(features['rv_5_10'].corr()-df_tar.corr(),0.05)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:11:44.861888Z","iopub.execute_input":"2021-09-28T13:11:44.862177Z","iopub.status.idle":"2021-09-28T13:11:50.559127Z","shell.execute_reply.started":"2021-09-28T13:11:44.862150Z","shell.execute_reply":"2021-09-28T13:11:50.558012Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"The order book helps traders to make more informed trading decisions by showing order imbalances that may provide clues to a stock’s direction in the very short term. A huge imbalance of buy orders against sell orders may indicate a move higher in the stock due to buying pressure, or vice versa. Traders can also use the order book to help pinpoint a stock’s potential support and resistance levels. A cluster of large buy orders at a specific price may indicate a level of support, while an abundance of sell orders at or near one price may suggest an area of resistance.\nRealized volatilities increase when those moves in either directions become more frequent. The function in the cell below is going to be used for visualizing individual order book time buckets and trying to find clues about their volatilities.","metadata":{}},{"cell_type":"code","source":"def visualize_book_time_bucket(stock_id, time_id):\n    \n    time_bucket = (df_train['stock_id'] == stock_id) & (df_train['time_id'] == time_id)\n    \n    target = df_train.loc[time_bucket, 'target'].iloc[0]\n    realized_volatility = df_train.loc[time_bucket, 'rv1'].iloc[0]\n    df_book = read_book_data('train', stock_id, sort=True, forward_fill=True)\n    df_book = df_book.set_index('seconds_in_bucket')\n    \n    df_book['wap1'] = (df_book['bid_price1'] * df_book['ask_size1'] + df_book['ask_price1'] * df_book['bid_size1']) /\\\n                      (df_book['bid_size1'] + df_book['ask_size1'])\n    df_book['wap2'] = (df_book['bid_price2'] * df_book['ask_size2'] + df_book['ask_price2'] * df_book['bid_size2']) /\\\n                      (df_book['bid_size2'] + df_book['ask_size2'])\n    \n    fig, axes = plt.subplots(figsize=(32, 30), nrows=2)\n    \n    axes[0].plot(df_book.loc[df_book['time_id'] == time_id, 'bid_price1'], label='bid_price1', lw=2, color='tab:green')\n    axes[0].plot(df_book.loc[df_book['time_id'] == time_id, 'ask_price1'], label='ask_price1', lw=2, color='tab:red')\n    axes[0].plot(df_book.loc[df_book['time_id'] == time_id, 'bid_price2'], label='bid_price2', alpha=0.3, color='tab:green')\n    axes[0].plot(df_book.loc[df_book['time_id'] == time_id, 'ask_price2'], label='ask_price2', alpha=0.3, color='tab:red')\n    axes[0].plot(df_book.loc[df_book['time_id'] == time_id, 'wap1'], label='wap1', lw=2, linestyle='--', color='tab:blue')\n    axes[0].plot(df_book.loc[df_book['time_id'] == time_id, 'wap2'], label='wap2', alpha=0.3, linestyle='--',  color='tab:blue')\n    \n    axes[1].plot(df_book.loc[df_book['time_id'] == time_id, 'bid_size1'], label='bid_size1', lw=2, color='tab:green')\n    axes[1].plot(df_book.loc[df_book['time_id'] == time_id, 'ask_size1'], label='ask_size1', lw=2, color='tab:red')\n    axes[1].plot(df_book.loc[df_book['time_id'] == time_id, 'bid_size2'], label='bid_size2', alpha=0.3, color='tab:green')\n    axes[1].plot(df_book.loc[df_book['time_id'] == time_id, 'ask_size2'], label='ask_size2', alpha=0.3, color='tab:red')\n    \n    for i in range(2):\n        axes[i].legend(prop={'size': 18})\n        axes[i].tick_params(axis='x', labelsize=20, pad=10)\n        axes[i].tick_params(axis='y', labelsize=20, pad=10)\n    axes[0].set_ylabel('price', size=20, labelpad=15)\n    axes[1].set_ylabel('size', size=20, labelpad=15)\n    \n    axes[0].set_title(\n        f'Prices of stock_id {stock_id} time_id {time_id} - Current Realized Volatility: {realized_volatility:.6f} - Next 10 minute Realized Volatility: {target:.6f}',\n        size=25,\n        pad=15\n    )\n    axes[1].set_title(\n        f'Sizes of stock_id {stock_id} time_id {time_id} - Current Realized Volatility: {realized_volatility:.6f} - Next 10 minute Realized Volatility: {target:.6f}',\n        size=25,\n        pad=15\n    )\n    \n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:11:50.560474Z","iopub.execute_input":"2021-09-28T13:11:50.560801Z","iopub.status.idle":"2021-09-28T13:11:50.580408Z","shell.execute_reply.started":"2021-09-28T13:11:50.560771Z","shell.execute_reply":"2021-09-28T13:11:50.579308Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"The most volatile time bucket in next 10 minute window is time **24600** from stock **77**. Current realized volatility of that time bucket is 0.02 which is roughly greater than 99.7% of other current realized volatilities. The time bucket was extremely volatile in current 10 minute window and it became the most volatile time bucket in next 10 minute window. Short-term realized volatilities are definitely correlated in this case. First of all, this time bucket is on a decreasing trend because large amount of most competitive ask sizes are always dominating most competitive bid sizes. Some traders were trying to sell lots of shares at most competitive price very frequently. In addition to that, this time bucket has more updates compared to others. Update frequency of time buckets might be correlated to realized volatility since more squared log return values greater than 0 will be summed.","metadata":{}},{"cell_type":"code","source":"visualize_book_time_bucket(stock_id=77, time_id=24600)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:11:50.581620Z","iopub.execute_input":"2021-09-28T13:11:50.581920Z","iopub.status.idle":"2021-09-28T13:12:01.324912Z","shell.execute_reply.started":"2021-09-28T13:11:50.581893Z","shell.execute_reply":"2021-09-28T13:12:01.323801Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"The most volatile time bucket in current 10 minute window is time **30128** from stock **30**. Realized volatility of the next 10 minute window for this time bucket is 0.039 which is roughly greater than %99.994 of other target values. In this case, this time bucket became less volatile in next 10 minute window and that suggests it is really hard to tell whether the realized volatility will be more or less in short-term, but it is expected to be closer to current realized volatility. This stock was extremely volatile in this time bucket because large amount of sell orders at most competitive prize dominated buy orders in first quarter and the opposite happened in second quarter. Later, buy/sell order domination kept switching places in smaller scales and that caused smaller spikes in weigted averaged price 1.","metadata":{}},{"cell_type":"code","source":"visualize_book_time_bucket(stock_id=30, time_id=30128)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:12:01.326291Z","iopub.execute_input":"2021-09-28T13:12:01.326627Z","iopub.status.idle":"2021-09-28T13:12:09.661733Z","shell.execute_reply.started":"2021-09-28T13:12:01.326595Z","shell.execute_reply":"2021-09-28T13:12:09.660751Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"The least volatile time bucket in next 10 minute window is time **8534** from stock **31**. The sizes of most competitive bid and ask orders were always balanced until the very end. Couple changes at the end caused a small decrease and increase in weighted average price 1. That was the only significant contribution to current realized volatility. This time bucket became almost 20 times less volatile in next 10 minute window.","metadata":{}},{"cell_type":"code","source":"visualize_book_time_bucket(stock_id=31, time_id=8534)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:12:09.662967Z","iopub.execute_input":"2021-09-28T13:12:09.663269Z","iopub.status.idle":"2021-09-28T13:12:19.404314Z","shell.execute_reply.started":"2021-09-28T13:12:09.663229Z","shell.execute_reply":"2021-09-28T13:12:19.403272Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"The least volatile time bucket in current 10 minute window is time **28959** from stock **31**. Prices and sizes of most competitive bid and ask orders are exteremely balanced in this entire time bucket. Sizes of most competitive bid level is shifted to a new state at the second half. That movement slightly increased the weighted averaged price 1, and that was the only thing contributed to current realized volatility. Stock 31 is very calm compared to other stocks. Sizes of most competitive bid and ask levels could be the reason of this phenomenon, or it could be an anomaly.","metadata":{}},{"cell_type":"code","source":"visualize_book_time_bucket(stock_id=31, time_id=28959)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:12:19.405677Z","iopub.execute_input":"2021-09-28T13:12:19.405991Z","iopub.status.idle":"2021-09-28T13:12:28.233250Z","shell.execute_reply.started":"2021-09-28T13:12:19.405961Z","shell.execute_reply":"2021-09-28T13:12:28.232430Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"markdown","source":"To wrap up, order book is very important as it gives information about buy and sell orders at most competitive levels. Target is also derived from order books of the stocks, so features extracted from them will be very useful. Features that are capturing the price/size imbalances might be useful for models to predict realized volatilities of next 10 minute window.","metadata":{}},{"cell_type":"code","source":"def get_book(stock_id):\n    df = read_book_data('train', stock_id, sort=True, forward_fill=True)\n    \n    df['wap1'] = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) /\\\n                      (df['bid_size1'] + df['ask_size1'])\n    \n    df['RV'] = df.groupby(['time_id'])['wap1'].apply(lambda x: np.log(x).diff().pow(2,fill_value=0).cumsum().pow(0.5) )\n    return df[['time_id', 'seconds_in_bucket','RV']]\n\ndef visualize_book_time_bucket(df_book, stock_id, time_id):\n    \n    time_bucket = (df_train['stock_id'] == stock_id) & (df_train['time_id'] == time_id)\n    \n    target = df_train.loc[time_bucket, 'target'].iloc[0]\n    realized_volatility = df_train.loc[time_bucket, 'rv1'].iloc[0]\n    plt.plot(df_book.loc[df_book['time_id'] == time_id, 'wap1'], label='wap1', lw=2, linestyle='--', color='tab:blue')\n    plt.plot(df_book.loc[df_book['time_id'] == time_id, 'wap2'], label='wap2', alpha=0.3, linestyle='--',  color='tab:blue')\n\n    plt.title(\n        f'Prices of stock_id {stock_id} time_id {time_id} - Current Realized Volatility: {realized_volatility:.6f} - Next 10 mins Realized Volatility: {target:.6f}',\n    )\n    \n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:12:28.234436Z","iopub.execute_input":"2021-09-28T13:12:28.234973Z","iopub.status.idle":"2021-09-28T13:12:28.245156Z","shell.execute_reply.started":"2021-09-28T13:12:28.234940Z","shell.execute_reply":"2021-09-28T13:12:28.244069Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"df_new = df_train.drop(['rv1', 'rv2', 'rv_0_5', 'rv_5_10'],axis=1)\ndf_new['seconds_in_bucket']=600\ndf_new.rename(columns={'target': 'RV'}, inplace=True)\n\nstock_id = 0\ndf_book = get_book(stock_id)\ndf_target = df_new[df_new['stock_id']== stock_id].drop('stock_id',axis=1)\ndf = pd.concat([df_book, df_target], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:12:28.246511Z","iopub.execute_input":"2021-09-28T13:12:28.246870Z","iopub.status.idle":"2021-09-28T13:12:39.634049Z","shell.execute_reply.started":"2021-09-28T13:12:28.246837Z","shell.execute_reply":"2021-09-28T13:12:39.632986Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:12:39.635619Z","iopub.execute_input":"2021-09-28T13:12:39.636064Z","iopub.status.idle":"2021-09-28T13:12:39.651106Z","shell.execute_reply.started":"2021-09-28T13:12:39.636022Z","shell.execute_reply":"2021-09-28T13:12:39.650137Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,15))\nfor time_id in df_train['time_id'].unique()[40:50]:\n    plt.plot(df.loc[df['time_id'] == time_id, 'seconds_in_bucket'], \n                df.loc[df['time_id'] == time_id, 'RV'])\n    \n    #visualize_book_time_bucket(df_book, stock_id=stock_id, time_id=time_id)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:12:39.652406Z","iopub.execute_input":"2021-09-28T13:12:39.652719Z","iopub.status.idle":"2021-09-28T13:12:40.071754Z","shell.execute_reply.started":"2021-09-28T13:12:39.652689Z","shell.execute_reply":"2021-09-28T13:12:40.070668Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"## 4.1 Model \n$X \\beta = Y$, where $X: t \\times k$, $\\beta: k\\times k$ and $Y: t \\times k$. $k$ is the number of stocks, $t$ is total time_ids.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\ndendrogram(Z, labels=df_train_tar.columns, orientation='top', \n           leaf_rotation=90);\nax = plt.gca()\nax.tick_params(axis='x', which='major', labelsize=12)\ncorr[[3,8,17,20,40,80,81,95,102,110,123]]\nlist_col = [3,8,17,20,40,80,81,95,102,110]","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:12:40.073267Z","iopub.execute_input":"2021-09-28T13:12:40.073857Z","iopub.status.idle":"2021-09-28T13:12:43.425866Z","shell.execute_reply.started":"2021-09-28T13:12:40.073811Z","shell.execute_reply":"2021-09-28T13:12:43.424826Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm\n\nimport numpy as np\nimport pandas as pd\n#pd.set_option('display.max_rows', 500)\n#pd.set_option('display.max_columns', 500)\n#pd.set_option('display.width', 1000)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy\nfrom scipy.stats import probplot\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n#https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html#least-squares-minimization-least-squares\n#https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.lsq_linear.html#scipy.optimize.lsq_linear\nfrom scipy.optimize import least_squares, lsq_linear\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:12:43.427327Z","iopub.execute_input":"2021-09-28T13:12:43.427679Z","iopub.status.idle":"2021-09-28T13:12:43.434292Z","shell.execute_reply.started":"2021-09-28T13:12:43.427645Z","shell.execute_reply":"2021-09-28T13:12:43.433111Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"\n\ndef Con_regress(X, y, lb, ub): # lb lower bound for variables b\n    # constrianted regression\n    res = lsq_linear(X, y, bounds=(lb,ub))\n    return res.x\n\ndef run_regress(X, Y, beta_var): # beta_var is how large we allow beta variates\n    k =Y.shape[1]\n    Beta = np.eye(X.shape[1],Y.shape[1])\n    for i in range(k):\n        Beta[:,i] = Con_regress(X,Y[:,i], Beta[:,i]-beta_var, Beta[:,i]+beta_var)\n    return Beta\n\nfeat = 'rv1' # rv2  rv_0_5   rv_5_10 \nlist_col = list(range(112)) # [3,8,17,20,40,80,81,95,102,110]\nX = features[feat].fillna(value=features[feat].mean()).to_numpy()[:,list_col]\npoly = PolynomialFeatures(degree=1)\n#X = poly.fit_transform(X)\n\nY = df_tar.fillna(value=df_tar.mean()).to_numpy()[:,list_col]\nX_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size=0.2, random_state=142)\nB = run_regress(X_train, Y_train, 4)\n\n\n\n# random_state =42 || 142\n# rv1 train_error:  0.28959 valid_error:  0.30731 || valid_error:  0.35573\n# rv2 train_error:  0.27871  valid_error:  0.29147 || valid_error:  0.44310\n# rv_0_5 , beta_var=0.5   train_error:  0.32927 valid_error:  0.35617  || beta_var=0.8 valid_error:  0.35551\n# rv_5_10 , beta_var=0.5  train_error:  0.30276 valid_error:  0.33233  || valid_error: 0.34606","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:12:43.436003Z","iopub.execute_input":"2021-09-28T13:12:43.436428Z","iopub.status.idle":"2021-09-28T13:12:45.671684Z","shell.execute_reply.started":"2021-09-28T13:12:43.436385Z","shell.execute_reply":"2021-09-28T13:12:45.670338Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"print('train_error: ', \"{:.5f}\".format(root_mean_squared_percentage_error(np.matmul(X_train,B), Y_train)),\n      'valid_error: ', \"{:.5f}\".format(root_mean_squared_percentage_error(np.matmul(X_valid,B), Y_valid)))","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:12:45.673482Z","iopub.execute_input":"2021-09-28T13:12:45.674354Z","iopub.status.idle":"2021-09-28T13:12:45.695378Z","shell.execute_reply.started":"2021-09-28T13:12:45.674298Z","shell.execute_reply":"2021-09-28T13:12:45.694228Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"- We saw when we only use 10 stocks as predictors, the error is reduced from 0.35 to 0.26, this means there are too many redundant stocks when using all stocks which introduced 0.1 variance error without reducing bias.","metadata":{}},{"cell_type":"code","source":"from sklearn.cross_decomposition import PLSRegression\nrmse, rmspe_train, rmspe_valid= [], [], []\nmax_comp = 15\nfor comp in range(1,max_comp):\n    pls2 = PLSRegression(n_components=comp, scale=False)\n    pls2.fit(X_train, Y_train)\n    rmse.append(pls2.score(X_train, Y_train))\n    Y_train_pred = pls2.predict(X_train)\n    Y_valid_pred = pls2.predict(X_valid)\n    rmspe_train.append(root_mean_squared_percentage_error(Y_train_pred, Y_train))\n    rmspe_valid.append(root_mean_squared_percentage_error(Y_valid_pred, Y_valid))\n\nplt.plot(range(1,max_comp),rmse)\nplt.show()   ","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:12:45.697279Z","iopub.execute_input":"2021-09-28T13:12:45.698070Z","iopub.status.idle":"2021-09-28T13:12:47.780672Z","shell.execute_reply.started":"2021-09-28T13:12:45.698014Z","shell.execute_reply":"2021-09-28T13:12:47.779713Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(1,max_comp),rmspe_train,label='train')\nplt.plot(range(1,max_comp),rmspe_valid,label='valid')\nplt.legend(prop={'size': 14})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:12:47.781913Z","iopub.execute_input":"2021-09-28T13:12:47.782206Z","iopub.status.idle":"2021-09-28T13:12:47.996598Z","shell.execute_reply.started":"2021-09-28T13:12:47.782175Z","shell.execute_reply":"2021-09-28T13:12:47.995643Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"- Conclusion: There is no magic here. PLS （partial lease square） regression has a comparable predicative power as linear models.  Even with a fancier model, the accuarcy is not improved when adding more components after we have chosen 6 principle components.","metadata":{}},{"cell_type":"markdown","source":"## 4.2 correlation preserve regression","metadata":{}},{"cell_type":"code","source":"def normalize(X):\n    X_stat = pd.concat([X.mean().to_frame().transpose(), X.std().to_frame().transpose()], ignore_index=True)\n    X = (X-X_stat.loc[0])/X_stat.loc[1]\n    return X, X_stat\nX, X_stat= normalize(features['rv_5_10']) # n by p : p=112 stocks, n=3830 samples\nY, Y_stat = normalize(df_tar) # target n by p\n\nX_corr =(X.T).dot(X)/3830\nY_corr =(Y.T).dot(Y)/3830\n#see_matrix(X.corr()-Y.corr(),0.05)\n#see_matrix(X_corr-Y_corr,0.05)\n# ||Q* X * Beta - Y ||","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:12:47.997920Z","iopub.execute_input":"2021-09-28T13:12:47.998226Z","iopub.status.idle":"2021-09-28T13:12:48.035161Z","shell.execute_reply.started":"2021-09-28T13:12:47.998196Z","shell.execute_reply":"2021-09-28T13:12:48.033903Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":"## 5. Trade\n\nTrade data represents the aggregation of all individual executed orders for corresponding stocks and time buckets. Size is the sum of the size in each individual order, price is aggregated as a weighted averaged price of all trades and order count is the number of unique trade orders taking place. For trade data, missing seconds_in_bucket implies no trade happening within that one second window.\n\nTrade data files are named as `trade_train.parquet` and `trade_test.parquet`, and they are parquet files partitioned by stock_id column. Partitioned by stock_id means those files can both read as whole or individual stocks. However, reading them as a single file is not easy as they will consume too much memory.\n\nThere are 5 columns in every trade data partition. The columns are:\n\n* `time_id` - ID of the time bucket\n* `seconds_in_bucket` - Number of seconds passed since the start of the bucket\n* `price` - Weighted average price of all executed trades happening in one second\n* `size` - Total number of traded shares happening in one second\n* `order_count` - Number of unique trade orders happening in one second\n\nTrade data has lots of missing seconds_in_bucket as it is more sparse than order book. Besides, there are some missing time buckets in trade data which means there weren't any trades in that 10 minute window. In addition to that, seconds_in_bucket doesn't necessarily start from 0 in trade data. Trade data can be still reshaped into a fixed size by adding missing time buckets and reindexing time steps to 600 seconds. Missing values can be filled with zeros afterwards since there weren't any trades at those one second windows.","metadata":{}},{"cell_type":"code","source":"def read_trade_data(df, dataset, stock_id, sort=False, zero_fill=False):\n        \n    trade_dtypes = {\n        'time_id': np.uint16,\n        'seconds_in_bucket': np.uint16,\n        'price': np.float32,\n        'size': np.uint16,\n        'order_count': np.uint16\n    }\n\n    df_trade = pd.read_parquet(f'../input/optiver-realized-volatility-prediction/trade_{dataset}.parquet/stock_id={stock_id}')\n    \n    if zero_fill:\n        stock_time_buckets = df.loc[df['stock_id'] == stock_id, 'time_id'].reset_index(drop=True)\n        missing_time_buckets = stock_time_buckets[~stock_time_buckets.isin(df_trade['time_id'])]\n        df_trade = df_trade.merge(missing_time_buckets, how='outer')\n    \n    if sort:\n        df_trade.sort_values(by=['time_id', 'seconds_in_bucket'], inplace=True)\n        \n    if zero_fill:\n        df_trade = df_trade.set_index(['time_id', 'seconds_in_bucket'])\n        df_trade = df_trade.reindex(\n            pd.MultiIndex.from_product([df_trade.index.levels[0], np.arange(0, 600)], names=['time_id', 'seconds_in_bucket']),\n        )\n        df_trade.fillna(0, inplace=True)\n        df_trade.reset_index(inplace=True)\n    \n    for column, dtype in trade_dtypes.items():\n        df_trade[column] = df_trade[column].astype(dtype)\n\n    return df_trade\n","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:12:48.036899Z","iopub.execute_input":"2021-09-28T13:12:48.037361Z","iopub.status.idle":"2021-09-28T13:12:48.053438Z","shell.execute_reply.started":"2021-09-28T13:12:48.037315Z","shell.execute_reply":"2021-09-28T13:12:48.051822Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":"Previously, realized volatilities were derived from weighted averaged price of the order book, but they can be derived from trade price as well since it is the weighted averaged price of the trade orders taking place. Realized volatilities derived from trade price are different from the previous ones because trade data is more sparse.","metadata":{}},{"cell_type":"code","source":"for stock_id in tqdm(sorted(df_train['stock_id'].unique())):\n            \n    df_trade = read_trade_data(df_train, 'train', stock_id)\n\n    # Realized volatility\n    for wap in [1, 2]:\n        df_trade['log_return_from_price'] = np.log(df_trade['price'] / df_trade.groupby('time_id')['price'].shift(1))\n        df_trade['squared_log_return_from_price'] = df_trade['log_return_from_price'] ** 2\n        df_trade['realized_volatility_from_price'] = np.sqrt(df_trade.groupby('time_id')['squared_log_return_from_price'].transform('sum'))\n        df_trade.drop(columns=['squared_log_return_from_price'], inplace=True)            \n        realized_volatilities = df_trade.groupby('time_id')['realized_volatility_from_price'].first().to_dict()\n        df_train.loc[df_train['stock_id'] == stock_id, 'realized_volatility_from_price'] = df_train[df_train['stock_id'] == stock_id]['time_id'].map(realized_volatilities)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:12:48.055178Z","iopub.execute_input":"2021-09-28T13:12:48.055644Z","iopub.status.idle":"2021-09-28T13:13:08.518697Z","shell.execute_reply.started":"2021-09-28T13:12:48.055599Z","shell.execute_reply":"2021-09-28T13:13:08.517705Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"Realized volatilies derived from trade price are used as baseline predictions. They score **0.380267** on training set. They can be used as predictors just like the realized volatilities derived from the order book.","metadata":{}},{"cell_type":"code","source":"realized_volatility_price_rmspe = root_mean_squared_percentage_error(df_train['target'], df_train['realized_volatility_from_price'])\nprint(f'Realized Volatility from price RMPSE: {realized_volatility_price_rmspe:.6}')","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:13:08.520124Z","iopub.execute_input":"2021-09-28T13:13:08.520583Z","iopub.status.idle":"2021-09-28T13:13:08.530971Z","shell.execute_reply.started":"2021-09-28T13:13:08.520521Z","shell.execute_reply":"2021-09-28T13:13:08.530300Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":"Trade data tells how many shares are traded at what weighted averaged price by how many unique traders for every time step. Those traded shares are the ones listed in the order book at the corresponding time step. Oscillations of trade price, size and order count might be an indicator of realized volatility of the next 10 minute window. Another important thing to consider is sparsity. Sparsity indicates that the stock is calm and there aren't any trades taking place which could be a strong indicator of low realized volatility. However, it may not be a predictor of the next 10 minute windows' realized volatility. The function in the cell below is going to be used for visualizing individual trade data time buckets and trying to find clues about their volatilities. It is important to visualize every sequence with and without zeros in order to understand both oscillations and sparsity.","metadata":{}},{"cell_type":"code","source":"def visualize_trade_time_bucket(stock_id, time_id):\n    \n    time_bucket = (df_train['stock_id'] == stock_id) & (df_train['time_id'] == time_id)\n    \n    target = df_train.loc[time_bucket, 'target'].iloc[0]\n    realized_volatility = df_train.loc[time_bucket, 'rv1'].iloc[0]\n    df_trade = read_trade_data(df_train, 'train', stock_id, sort=True, zero_fill=True)\n    df_trade = df_trade.set_index('seconds_in_bucket')\n    \n    fig, axes = plt.subplots(figsize=(32, 70), nrows=6)\n    \n    axes[0].plot(df_trade.loc[(df_trade['time_id'] == time_id) & (df_trade['price'] != 0), 'price'], label='price without zeros', lw=2, linestyle='-', color='tab:blue')\n    axes[1].plot(df_trade.loc[(df_trade['time_id'] == time_id), 'price'], label='price with zeros', lw=2, linestyle='--', color='tab:blue')\n    axes[2].plot(df_trade.loc[(df_trade['time_id'] == time_id) & (df_trade['price'] != 0), 'size'], label='size without zeros', lw=2, linestyle='-', color='tab:blue')\n    axes[3].plot(df_trade.loc[(df_trade['time_id'] == time_id), 'size'], label='size with zeros', lw=2, linestyle='--', color='tab:blue')\n    axes[4].plot(df_trade.loc[(df_trade['time_id'] == time_id) & (df_trade['price'] != 0), 'order_count'], label='order_count without zeros', lw=2, linestyle='-', color='tab:blue')\n    axes[5].plot(df_trade.loc[(df_trade['time_id'] == time_id), 'order_count'], label='order_count with zeros', lw=2, linestyle='--', color='tab:blue')\n    \n    for i in range(6):\n        axes[i].legend(prop={'size': 18})\n        axes[i].tick_params(axis='x', labelsize=20, pad=10)\n        axes[i].tick_params(axis='y', labelsize=20, pad=10)\n    axes[0].set_ylabel('price', size=20, labelpad=15)\n    axes[1].set_ylabel('price', size=20, labelpad=15)\n    axes[2].set_ylabel('size', size=20, labelpad=15)\n    axes[3].set_ylabel('size', size=20, labelpad=15)\n    axes[4].set_ylabel('order_count', size=20, labelpad=15)\n    axes[5].set_ylabel('order_count', size=20, labelpad=15)\n    \n    axes[0].set_title(\n        f'Price of stock_id {stock_id} time_id {time_id} without zeros - Current Realized Volatility: {realized_volatility:.6f} - Next 10 minute Realized Volatility: {target:.6f}',\n        size=25,\n        pad=15\n    )\n    axes[1].set_title(\n        f'Price of stock_id {stock_id} time_id {time_id} with zeros - Current Realized Volatility: {realized_volatility:.6f} - Next 10 minute Realized Volatility: {target:.6f}',\n        size=25,\n        pad=15\n    )\n    axes[2].set_title(\n        f'Size of stock_id {stock_id} time_id {time_id} without zeros - Current Realized Volatility: {realized_volatility:.6f} - Next 10 minute Realized Volatility: {target:.6f}',\n        size=25,\n        pad=15\n    )\n    axes[3].set_title(\n        f'Size of stock_id {stock_id} time_id {time_id} with zeros - Current Realized Volatility: {realized_volatility:.6f} - Next 10 minute Realized Volatility: {target:.6f}',\n        size=25,\n        pad=15\n    )\n    axes[4].set_title(\n        f'Order count of stock_id {stock_id} time_id {time_id} without zeros - Current Realized Volatility: {realized_volatility:.6f} - Next 10 minute Realized Volatility: {target:.6f}',\n        size=25,\n        pad=15\n    )\n    axes[5].set_title(\n        f'Order count of stock_id {stock_id} time_id {time_id} with zeros - Current Realized Volatility: {realized_volatility:.6f} - Next 10 minute Realized Volatility: {target:.6f}',\n        size=25,\n        pad=15\n    )\n    \n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:13:08.532101Z","iopub.execute_input":"2021-09-28T13:13:08.532520Z","iopub.status.idle":"2021-09-28T13:13:08.551964Z","shell.execute_reply.started":"2021-09-28T13:13:08.532481Z","shell.execute_reply":"2021-09-28T13:13:08.550863Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"markdown","source":"As seen before, the most volatile time bucket in next 10 minute window is time **24600** from stock **77**. Current realized volatility of that time bucket is 0.02 which is roughly greater than 99.7% of other current realized volatilities. Trade price without zeros is almost perfectly correlated with weighted averaged price from the order book and trade price with zeros shows that there are trades taking place very frequently. Size shows that there are some extreme amounts of shares traded at certain times by one or multiple traders. Finally, order count shows that there are large number of unique trade orders from time to time regardless of the total size and price.","metadata":{}},{"cell_type":"code","source":"visualize_trade_time_bucket(stock_id=77, time_id=24600)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:13:08.553451Z","iopub.execute_input":"2021-09-28T13:13:08.553852Z","iopub.status.idle":"2021-09-28T13:13:16.371710Z","shell.execute_reply.started":"2021-09-28T13:13:08.553818Z","shell.execute_reply":"2021-09-28T13:13:16.370807Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":"As seen before, the least volatile time bucket in next 10 minute window is time **8534** from stock **31**. There are only 4 trades took place in this time bucket which is why it's current realized volatility and next 10 minute window realized volatility is very low. Regardless of the values, sparsity is the obvious reason in this case. ","metadata":{}},{"cell_type":"code","source":"visualize_trade_time_bucket(stock_id=31, time_id=8534)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:13:16.372902Z","iopub.execute_input":"2021-09-28T13:13:16.373368Z","iopub.status.idle":"2021-09-28T13:13:24.549651Z","shell.execute_reply.started":"2021-09-28T13:13:16.373335Z","shell.execute_reply":"2021-09-28T13:13:24.548759Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":"There are some time buckets with zero trades exist in trade data. Time **62** from stock **37** is an example of those time buckets. We should use data from book_data to replace the zero.","metadata":{}},{"cell_type":"code","source":"visualize_trade_time_bucket(stock_id=37, time_id=62)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:13:24.550787Z","iopub.execute_input":"2021-09-28T13:13:24.551207Z","iopub.status.idle":"2021-09-28T13:13:31.823007Z","shell.execute_reply.started":"2021-09-28T13:13:24.551176Z","shell.execute_reply":"2021-09-28T13:13:31.821953Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"markdown","source":"## 6. Feature Engineering","metadata":{}},{"cell_type":"code","source":"# To Be Continued","metadata":{"execution":{"iopub.status.busy":"2021-09-28T13:13:31.824523Z","iopub.execute_input":"2021-09-28T13:13:31.824940Z","iopub.status.idle":"2021-09-28T13:13:31.829240Z","shell.execute_reply.started":"2021-09-28T13:13:31.824901Z","shell.execute_reply":"2021-09-28T13:13:31.828248Z"},"trusted":true},"execution_count":87,"outputs":[]}]}